## Compute Resources

setup_meld.py:

Pure setup (build system, write DataStore). CPU-only is fine; GPU gives negligible benefit here.
launch_remd_multiplex:

Performs MD integration via OpenMM; this is the GPU‑intensive part.
Nonbonded force calculations (even with implicit solvent) dominate cost and are 5–30× faster on CUDA GPUs vs CPU.
Replica‑exchange bookkeeping (ladder/adaptor, state swaps) is light and not GPU critical.



az ml compute create  --resource-group rg-mayo-2 --workspace-name mlw-1  --name meld-v100   --type amlcompute   --min-instances 0   --max-instances 40   --
size Standard_NC6s_v3



az ml compute create \
  --resource-group rg-mayo-2 \
  --workspace-name mlw-1 \
  --name meld-v100-spot \
  --type amlcompute \
  --size Standard_NC6s_v3 \
  --min-instances 0 \
  --max-instances 10 \
  --ssh-public-access-enabled true \
  --admin-username azureuser \
  --ssh-key-value ~/.ssh/id_rsa.pub



Standard_NC80adis_H100_v5 has 2x H100 NVL (≈94–95 GB each), 80 vCPUs total, 640 GB RAM.
Approx per‑GPU share (just a planning heuristic):

vCPUs: 40 per GPU
RAM: 320 GB per GPU
GPU memory: ~95 GB per GPU (reported 95,830 MiB)
Local (ephemeral) disk 256 GB is shared; no fixed per‑GPU split (treat as common scratch)
Practical guidance:

Run one main simulation rank per GPU; if using OpenMM / MELD with minor CPU helpers set OMP_NUM_THREADS (or MKL_NUM_THREADS) to 8–16, not full 40, to leave headroom for I/O and the other rank.

## Flag summary (selected)
- --mpi-gpus 0,1          Run coordinated REMD across listed GPUs.
- --scratch-blocks        Write per-rank NetCDF Blocks in rank-specific scratch dirs; merge on exit (prevents block_000000.nc contention).
- --force-reinit          Backup and recreate Data directory (use once after errors or to start fresh).
- --clean-data            Backup existing Data then start fresh (less aggressive than force if already clean of partial files).
- --gpus 0,1              Launch independent (non-MPI) runs on listed GPUs.
- --multi-gpus 0,1        Single multiplex process with multiple GPUs visible (no MPI, avoids per-rank duplication).


## Recipe
1. Install Miniconda
2. Run simulation (coordinated multi-GPU REMD via MPI):
   ```nohup bash -lc "./run_meld.sh --mpi-gpus 0,1 --scratch-blocks" > remd_mpi_$(date +%Y%m%d_%H%M%S).log 2>&1 &```

## MPI Multi-GPU Launch

New script `run_mpi_meld.sh` launches one MELD replica per GPU via MPI:

```
./run_mpi_meld.sh                # auto-detect GPUs, mpirun -np=min(30, n_gpus)
./run_mpi_meld.sh --gpus 0,1,2,3  # restrict to listed GPUs
./run_mpi_meld.sh --gpus 0,1 --resume
./run_mpi_meld.sh --np 16         # set explicit MPI ranks (round‑robin GPUs)
./run_mpi_meld.sh --resume        # skip rebuilding Data store if present
./run_mpi_meld.sh --dry-run       # show command only
```

Pre-flight sanity checks executed:
* `nvidia-smi -L`
* `mpirun --version`

It invokes:

```
mpirun -np <N> bash run_gpu_meld.sh python launch_remd.py
```

Where `run_gpu_meld.sh` assigns each rank a distinct `CUDA_VISIBLE_DEVICES` (local rank).

`launch_remd.py` loads `Data/data_store.dat` generated by `setup_meld.py` and advances the
replica-exchange until completion (based on stored runner `max_steps`).

Recommendation: keep one replica per GPU for best exchange efficiency; oversubscription usually hurts performance.

### GaMELD Patch Integration
If you place a `gameld.py` file in the local `patches/` directory it will now be copied over (on environment activation inside `run_mpi_meld.sh`) to override the installed `meld.system.gameld` module. This enables rapid iteration on GaMELD threshold logic without rebuilding the MELD wheel. To disable all patching use `--no-comm-patch`; to require the patch (fail if not applied) add `--require-comm-patch`.


chmod +x run_gpu_meld.sh

# Example: 2 nodes × 4 GPUs/node = 8 replicas
mpirun -np 8 \
  --map-by ppr:4:node:PE=1 \
  --bind-to core \
  -x CUDA_DEVICE_ORDER \
  ./run_gpu_meld.sh launch_remd --steps 200000

# Example: 1 nodes × 2 GPUs/node = 2 replicas
mpirun -np 2 \
  --map-by ppr:1:node:PE=1 \
  --bind-to core \
  -x CUDA_DEVICE_ORDER \
  ./run_gpu_meld.sh launch_remd --steps 200000



python launch_remd.py --setup \
  --templates TEMPLATES \
  --pep-restraints protein_pep_all.dat \
  --prot-restraints protein_contacts.dat \
  --n-replicas 30 --n-steps 2000 --block-size 100 \
  --tmin 300 --tmax 50




How To Re-run (foreground example)
bash -lc "./run_meld.sh --multi-gpus 0,1"

Background run
nohup bash -lc "./run_meld.sh --multi-gpus 0,1 --scratch-blocks --background" > remd_multigpu_$(date +%Y%m%d_%H%M%S).log 2>&1 &

Verify It’s Running
ps -f | grep launch_remd_multiplex | grep -v grep
nvidia-smi
grep 'Running replica exchange step' Runs/run/multigpu_*/remd.log | head

   ```nohup bash -lc "./run_meld.sh --multi-gpus 0,1 --scratch-blocks" > remd_multigpu_$(date +%Y%m%d_%H%M%S).log 2>&1 &```


  ```nohup bash -lc "./run_mpi_meld.sh --gpus 0,1,2,3 --np 30  --allow-oversubscribe --verify-comm --meld-debug-comm --require-comm-patch" > remd_mpigpu_$(date +%Y%m%d_%H%M%S).log 2>&1 &```

  ```nohup bash -lc "./run_mpi_meld.sh --gpus 0,1,2,3 --np 30  --allow-oversubscribe --verify-comm --meld-debug-comm --require-comm-patch --dry-run" > remd_mpigpu_$(date +%Y%m%d_%H%M%S).log 2>&1 &```

./run_mpi_meld.sh --gpus 0,1 --np 2 --meld-debug-comm --verify-comm --require-comm-patch --dry-run

    ```nohup bash -lc "./run_mpi_meld.sh --gpus 0,1,2,3 --np 30 --auto-install-mpi" > remd_mpigpu_$(date +%Y%m%d_%H%M%S).log 2>&1 &```

  ```nohup bash -lc './run_mpi_meld.sh --gpus 0,1 --np 2 --multiplex-factor 15' \
  > remd_mpi_mux30r2g_$(date +%Y%m%d_%H%M%S).log 2>&1 &```

```nohup bash -lc './run_mpi_meld.sh --gpus 0,1 --np 2 --scratch-blocks --multiplex-factor 16 --allow-partial' \
  > remd_mpi_mux_partial_$(date +%Y%m%d_%H%M%S).log 2>&1 &```

## Extracting trajectory
 conda activate d3-meld-2-env
extract_trajectory extract_traj_dcd --replica 0 trajectory.00.dcd

If you previously saw: AttributeError: 'RunOptions' object has no attribute 'solvation'
- Both sitecustomize.py and usercustomize.py now patch MELD.
- Verify patch:
  SOLVATION_PATCH_DEBUG=1 python -c "import meld; print('Has solvation:', hasattr(meld.RunOptions,'solvation'))"
- Ensure repo root is on PYTHONPATH (run from root or: export PYTHONPATH=$PWD:$PYTHONPATH).
- Then:
  conda activate d3-meld-2-env
  extract_trajectory extract_traj_dcd --replica 0 trajectory.00.dcd
  extract_trajectory follow_dcd --replica 0 follow.00.dcd
# Visualize (RMSD + optional interactive)
python visualize_dcd.py --top prot_tleap.pdb trajectory.00.dcd follow.00.dcd
python visualize_dcd.py --top prot_tleap.pdb --selection "name CA and resid 69-91" trajectory.00.dcd
# Interactive (if nglview installed)
python visualize_dcd.py --top prot_tleap.pdb --show trajectory.00.dcd

python upload_files.py
rmsd_all_overlay.png  rmsd_follow.00.dcd.png  rmsd_trajectory.00.dcd.png

## Troubleshooting
- Check GPU visibility:
  nvidia-smi

## Troubleshooting (OpenMM Upgrade)

If you see:
```
error: uninstall-distutils-installed-package
Cannot uninstall OpenMM 7.7.0
```
Cause: pip tries to replace a conda-installed OpenMM (7.7.0) with >=8.0 and refuses to uninstall the distutils-style package.

Resolution (preferred):
```
conda remove openmm
conda install -c conda-forge "openmm>=8.0"
```

If you must let pip manage it (not recommended when using conda):
```
pip install --ignore-installed "openmm>=8.0"
```

Script behavior:
- `run_mpi_meld.sh` now auto-detects OpenMM<8 and performs a conda upgrade first.
- It removes `openmm` from the pip requirements pass after a successful conda upgrade to avoid conflicts.
- Override with `USE_PIP_OPENMM=1` to force pip to manage OpenMM.

Verify:
```
python -c "import openmm,sys;print(openmm.__version__)"
```
Expect >= 8.0.

## Monitoring

grep 'Running replica exchange step ' ./remd.log | tail -n 40

## Slurm / srun Usage

For clusters using Slurm you can launch MELD REMD (or multiplex) with the provided helper scripts.

Quick interactive launch (2 ranks, 1 GPU each, multiplex):
```
./run_meld_slurm.sh --ntasks 2 --gpus-per-task 1 --partition gpu --time 02:00:00 --mode multiplex --debug
```

Core components:
- `run_meld_slurm.sh` parses options, activates the Conda env, then invokes `srun` with `--mpi=pmix_v3`.
- `srun_remd_wrapper.sh` runs on every rank. Rank 0 ensures `Data/data_store.dat` exists (running `setup_meld.py` if needed); other ranks wait until it appears.

Important notes:
- Many MELD builds still run the full set of replicas per rank when using `launch_remd_multiplex`. Increase `--ntasks` only if your build partitions replicas; otherwise keep to 1 rank per GPU only when using independent jobs.
- Use `--mode remd` to switch from multiplex to the non-multiplex launcher (`launch_remd`).
- Set `SCRATCH_BLOCKS=1` environment variable (or `--scratch-blocks` flag) if you observe HDF5 block contention.

Example sbatch script snippet:
```
#!/usr/bin/env bash
#SBATCH -J meld
#SBATCH -p gpu
#SBATCH -t 04:00:00
#SBATCH -N 1
#SBATCH -n 2
#SBATCH --gpus-per-task=1

module load cuda/12.2  # if required by cluster
source $(conda info --base)/etc/profile.d/conda.sh
conda activate d3-meld-2-env

./run_meld_slurm.sh --ntasks 2 --gpus-per-task 1 --mode multiplex --debug
```

Check per-rank logs under `Logs/slurm_r<rank>/remd_rank<rank>.log`.

## Blob Upload (Azure)

To keep this README clean and free of environment‑specific identifiers, detailed usage instructions, examples, and placeholders for `blob_upload.py` have been moved to `docs/blob_upload_usage.md`.

Quick essentials:
- Script: `blob_upload.py`
- Auth: DefaultAzureCredential (interactive / SP / MI) or explicit Managed Identity via `--managed-identity`.
- Provide storage context via `--account-name` + `--container` (or `--account-url`).
- Optional: `.env` with `AZURE_TENANT_ID`, `ACCOUNT_NAME`, `BLOB_CONTAINER`.

View full instructions:

```
cat docs/blob_upload_usage.md
```

```shell
python -m venv .venv 
source .venv/bin/activate
```

```shell
pip install -r requirements.txt
```

Minimal example (placeholders):
```bash
ACCOUNT_NAME=yourstorageacct \
BLOB_CONTAINER=your-container \
python blob_upload.py --account-name "$ACCOUNT_NAME" --container "$BLOB_CONTAINER" --path path/to/file.dat
```


Managed Identity example:
```bash
ACCOUNT_NAME=yourstorageacct \
BLOB_CONTAINER=your-container \
python blob_upload.py --managed-identity --account-name "$ACCOUNT_NAME" --container "$BLOB_CONTAINER" --path output_dir
```

See the docs file for: concurrency tuning, conditional uploads, content-type detection, tenant validation, exit codes, troubleshooting, and performance tips.



```mermaid
graph TD
    subgraph Node1[Node-A100]
        GPU0_1[GPU 0]
        GPU1_1[GPU 1]
        GPU2_1[GPU 2]
        GPU3_1[GPU 3]

        GPU0_1 --> L13[Leader 13]
        GPU0_1 --> W37[Worker 37]
        GPU0_1 --> W38[Worker 38]
        GPU0_1 --> W39[Worker 39]

        GPU1_1 --> L14[Leader 14]
        GPU1_1 --> W40[Worker 40]
        GPU1_1 --> W41[Worker 41]
        GPU1_1 --> W42[Worker 42]

        GPU2_1 --> L15[Leader 15]
        GPU2_1 --> W43[Worker 43]
        GPU2_1 --> W44[Worker 44]
        GPU2_1 --> W45[Worker 45]

        GPU3_1 --> L16[Leader 16]
        GPU3_1 --> W46[Worker 46]
        GPU3_1 --> W47[Worker 47]
        GPU3_1 --> W48[Worker 48]
    end

    subgraph Node2[Node-A100]
        GPU0_2[GPU 0]
        GPU1_2[GPU 1]
        GPU2_2[GPU 2]
        GPU3_2[GPU 3]

        GPU0_2 --> L9[Leader 9]
        GPU0_2 --> W25[Worker 25]
        GPU0_2 --> W26[Worker 26]
        GPU0_2 --> W27[Worker 27]

        GPU1_2 --> L10[Leader 10]
        GPU1_2 --> W28[Worker 28]
        GPU1_2 --> W29[Worker 29]
        GPU1_2 --> W30[Worker 30]

        GPU2_2 --> L11[Leader 11]
        GPU2_2 --> W31[Worker 31]
        GPU2_2 --> W32[Worker 32]
        GPU2_2 --> W33[Worker 33]

        GPU3_2 --> L12[Leader 12]
        GPU3_2 --> W34[Worker 34]
        GPU3_2 --> W35[Worker 35]
        GPU3_2 --> W36[Worker 36]
    end

    subgraph Node3[Node-A100]
        GPU0_3[GPU 0]
        GPU1_3[GPU 1]
        GPU2_3[GPU 2]
        GPU3_3[GPU 3]

        GPU0_3 --> L5[Leader 5]
        GPU0_3 --> W13[Worker 13]
        GPU0_3 --> W14[Worker 14]
        GPU0_3 --> W15[Worker 15]

        GPU1_3 --> L6[Leader 6]
        GPU1_3 --> W16[Worker 16]
        GPU1_3 --> W17[Worker 17]
        GPU1_3 --> W18[Worker 18]

        GPU2_3 --> L7[Leader 7]
        GPU2_3 --> W19[Worker 19]
        GPU2_3 --> W20[Worker 20]
        GPU2_3 --> W21[Worker 21]

        GPU3_3 --> L8[Leader 8]
        GPU3_3 --> W22[Worker 22]
        GPU3_3 --> W23[Worker 23]
        GPU3_3 --> W24[Worker 24]
    end

    subgraph Node4[Node-A100]
        GPU0_4[GPU 0]
        GPU1_4[GPU 1]
        GPU2_4[GPU 2]
        GPU3_4[GPU 3]

        GPU0_4 --> L1[Leader 1]
        GPU0_4 --> W1[Worker 1]
        GPU0_4 --> W2[Worker 2]
        GPU0_4 --> W3[Worker 3]

        GPU1_4 --> L2[Leader 2]
        GPU1_4 --> W4[Worker 4]
        GPU1_4 --> W5[Worker 5]
        GPU1_4 --> W6[Worker 6]

        GPU2_4 --> L3[Leader 3]
        GPU2_4 --> W7[Worker 7]
        GPU2_4 --> W8[Worker 8]
        GPU2_4 --> W9[Worker 9]

        GPU3_4 --> L4[Leader 4]
        GPU3_4 --> W10[Worker 10]
        GPU3_4 --> W11[Worker 11]
        GPU3_4 --> W12[Worker 12]
    end
````